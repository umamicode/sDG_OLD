{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f575f57-77ba-41fc-b358-3b8c1b457305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/simclr/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, USPS, SVHN, CIFAR10, STL10\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "from tools.autoaugment import SVHNPolicy, CIFAR10Policy\n",
    "from tools.randaugment import RandAugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7003b425-81b8-4419-b336-6f87ba949dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.environ['HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97447a9f-8299-4fa1-b5b9-82e7b9e3363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(split='train', translate=None, twox=False, ntr=None, autoaug=None, channels=3):\n",
    "    '''\n",
    "        autoaug == 'AA', AutoAugment\n",
    "                   'FastAA', Fast AutoAugment\n",
    "                   'RA', RandAugment\n",
    "        channels == 3 return by default rgb 3 channel image\n",
    "                    1 Return a single channel image\n",
    "    '''\n",
    "    path = f'data/mnist-{split}.pkl'\n",
    "    if not os.path.exists(path):\n",
    "        dataset = MNIST(f'{HOME}/.pytorch/MNIST', train=(split=='train'), download=True)\n",
    "        x, y = dataset.data, dataset.targets\n",
    "        if split=='train':\n",
    "            #[TODO]- Why should we only use 10k images?\n",
    "            #x, y = x, y\n",
    "            x, y = x[0:10000], y[0:10000]\n",
    "        x = torch.tensor(resize_imgs(x.numpy(), 32))\n",
    "        x = (x.float()/255.).unsqueeze(1).repeat(1,3,1,1)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump([x, y], f)\n",
    "    with open(path, 'rb') as f:\n",
    "        x, y = pickle.load(f)\n",
    "        if channels == 1:\n",
    "            x = x[:,0:1,:,:]\n",
    "    \n",
    "    if ntr is not None:\n",
    "        x, y = x[0:ntr], y[0:ntr]\n",
    "    \n",
    "    # Without Data Augmentation\n",
    "    if (translate is None) and (autoaug is None):\n",
    "        dataset = TensorDataset(x, y)\n",
    "        return dataset\n",
    "    \n",
    "    # Data Augmentation Pipeline\n",
    "    transform = [transforms.ToPILImage()]\n",
    "    if translate is not None:\n",
    "        transform.append(transforms.RandomAffine(0, [translate, translate]))\n",
    "    if autoaug is not None:\n",
    "        if autoaug == 'AA':\n",
    "            transform.append(SVHNPolicy())\n",
    "        elif autoaug == 'RA':\n",
    "            transform.append(RandAugment(3,4))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(transform)\n",
    "    dataset = myTensorDataset(x, y, transform=transform, twox=twox)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27304e8f-6841-49ba-9d6c-43b4962abfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10(split='train', translate=None, twox=False, ntr=None, autoaug=None, channels=3):\n",
    "    '''\n",
    "        autoaug == 'AA', AutoAugment\n",
    "                   'FastAA', Fast AutoAugment\n",
    "                   'RA', RandAugment\n",
    "        channels == 3 return by default rgb 3 channel image\n",
    "                    1 Return a single channel image\n",
    "    '''\n",
    "    path = f'data/cifar10-{split}.pkl'\n",
    "    cifar10_transforms_train= transforms.Compose([transforms.Resize((32,32))]) #224,224\n",
    "    if not os.path.exists(path):\n",
    "        dataset = CIFAR10(f'{HOME}/.pytorch/CIFAR10', train=(split=='train'), download=True, transform= cifar10_transforms_train)\n",
    "        x, y = dataset.data, dataset.targets\n",
    "        \n",
    "        #Only Select First 10k images as train\n",
    "        #if split=='train':\n",
    "        #    x, y = x[0:10000], y[0:10000]\n",
    "        \n",
    "        #[TODO] - solve -> AttributeError: 'numpy.ndarray' object has no attribute 'numpy'\n",
    "        #x = torch.tensor(resize_imgs(x.numpy(), 32))\n",
    "        #x = torch.tensor(resize_imgs_dkcho(x, 32)) # x-> torch.Size([10000, 32, 32, 3])\n",
    "        x= torch.tensor(x)\n",
    "        x = (x.float()/255.)#.unsqueeze(1).repeat(1,3,1,1)  #<class 'torch.Tensor'>\n",
    "        x= x.permute(0,3,1,2) #[batchsize,w,h,channel] -> [batchsize, channel, w,h]\n",
    "        y = torch.tensor(y)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump([x, y], f)\n",
    "    with open(path, 'rb') as f:\n",
    "        x, y = pickle.load(f)\n",
    "        if channels == 1:\n",
    "            x = x[:,0:1,:,:]\n",
    "    \n",
    "    if ntr is not None:\n",
    "        x, y = x[0:ntr], y[0:ntr]\n",
    "    \n",
    "    # Without Data Augmentation\n",
    "    if (translate is None) and (autoaug is None):\n",
    "        dataset = TensorDataset(x, y)\n",
    "        return dataset\n",
    "    \n",
    "    # Data Augmentation Pipeline\n",
    "    transform = [transforms.ToPILImage()]\n",
    "    if translate is not None:\n",
    "        transform.append(transforms.RandomAffine(0, [translate, translate]))\n",
    "    if autoaug is not None:\n",
    "        if autoaug == 'AA':\n",
    "            transform.append(CIFAR10Policy()) #originally SVHNPolicy()\n",
    "        elif autoaug == 'RA':\n",
    "            transform.append(RandAugment(3,4))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(transform)\n",
    "    dataset = myTensorDataset(x, y, transform=transform, twox=twox)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_usps(split='train', channels=3):\n",
    "    path = f'data/usps-{split}.pkl'\n",
    "    if not os.path.exists(path):\n",
    "        dataset = USPS(f'{HOME}/.pytorch/USPS', train=(split=='train'), download=True)\n",
    "        x, y = dataset.data, dataset.targets\n",
    "        x = torch.tensor(resize_imgs(x, 32))\n",
    "        x = (x.float()/255.).unsqueeze(1).repeat(1,3,1,1)\n",
    "        y = torch.tensor(y)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump([x, y], f)\n",
    "    with open(path, 'rb') as f:\n",
    "        x, y = pickle.load(f)\n",
    "        if channels == 1:\n",
    "            x = x[:,0:1,:,:]\n",
    "    dataset = TensorDataset(x, y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b43914-8bfc-428d-b0f9-8c75119b6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10c(split='train', translate=None, twox=False, ntr=None, autoaug=None, channels=3):\n",
    "    path = f'data/cifar10c-{split}.pkl'\n",
    "    cifar10_transforms_train= transforms.Compose([transforms.Resize((32,32))]) #224,224\n",
    "    if not os.path.exists(path):\n",
    "        dataset = tfds.as_numpy(tfds.load('cifar10_corrupted', split= split, shuffle_files= True, batch_size= -1))\n",
    "        x, y = dataset['image'], dataset['label']\n",
    "        #dataset = CIFAR10(f'{HOME}/.pytorch/CIFAR10', train=(split=='train'), download=True, transform= cifar10_transforms_train)\n",
    "        #x, y = dataset.data, dataset.targets\n",
    "        \n",
    "        #Only Select First 10k images as train\n",
    "        #if split=='train':\n",
    "        #    x, y = x[0:10000], y[0:10000]\n",
    "        \n",
    "        #[TODO] - solve -> AttributeError: 'numpy.ndarray' object has no attribute 'numpy'\n",
    "        #x = torch.tensor(resize_imgs(x.numpy(), 32))\n",
    "        #x = torch.tensor(resize_imgs_dkcho(x, 32)) # x-> torch.Size([10000, 32, 32, 3])\n",
    "        x= torch.tensor(x)\n",
    "        x = (x.float()/255.)#.unsqueeze(1).repeat(1,3,1,1)  #<class 'torch.Tensor'>\n",
    "        x= x.permute(0,3,1,2) #[batchsize,w,h,channel] -> [batchsize, channel, w,h]\n",
    "        y = torch.tensor(y)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump([x, y], f)\n",
    "    with open(path, 'rb') as f:\n",
    "        x, y = pickle.load(f)\n",
    "        if channels == 1:\n",
    "            x = x[:,0:1,:,:]\n",
    "    \n",
    "    if ntr is not None:\n",
    "        x, y = x[0:ntr], y[0:ntr]\n",
    "    \n",
    "    # Without Data Augmentation\n",
    "    if (translate is None) and (autoaug is None):\n",
    "        dataset = TensorDataset(x, y)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3954e302-89df-4e40-921b-5fe68ea821f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_svhn(split='train', channels=3):\n",
    "    dataset = SVHN(f'{HOME}/.pytorch/SVHN', split=split, download=True)\n",
    "    x, y = dataset.data, dataset.labels\n",
    "    x = x.astype('float32')/255.\n",
    "    x, y = torch.tensor(x), torch.tensor(y)\n",
    "    if channels == 1:\n",
    "        x = x.mean(1, keepdim=True)\n",
    "    dataset = TensorDataset(x, y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df071344-67f6-4f61-878e-1576b540193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pacs(split='train', translate=None, twox=False, ntr=None, autoaug=None, channels=3):\n",
    "    #PACS Dataset\n",
    "    NUM_CLASSES = 7      # 7 classes for each domain: 'dog', 'elephant', 'giraffe', 'guitar', 'horse', 'house', 'person'\n",
    "    DATASETS_NAMES = ['photo', 'art', 'cartoon', 'sketch']\n",
    "    CLASSES_NAMES = ['Dog', 'Elephant', 'Giraffe', 'Guitar', 'Horse', 'House', 'Person']\n",
    "    DIR_PHOTO = './data/PACS/photo'\n",
    "    DIR_ART = './data/PACS/art_painting'\n",
    "    DIR_CARTOON = './data/PACS/cartoon'\n",
    "    DIR_SKETCH = './data/PACS/sketch'\n",
    "\n",
    "    path = f'data/pacs-{split}.pkl'\n",
    "\n",
    "    pacs_convertor= {'train':DIR_PHOTO, 'photo':DIR_PHOTO, 'art':DIR_ART, 'cartoon':DIR_CARTOON, 'sketch':DIR_SKETCH}\n",
    "    \n",
    "    pacs_transforms_train= transforms.Compose([transforms.ToTensor(),transforms.Resize((224,224))]) #224,224\n",
    "    if not os.path.exists(path):\n",
    "        dataset= torchvision.datasets.ImageFolder(pacs_convertor[split], transform=pacs_transforms_train)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset,batch_size=len(dataset),drop_last=True)\n",
    "        x, y= next(iter(train_loader))\n",
    "        #x, y = dataset.image, dataset.label\n",
    "        x= torch.tensor(x)\n",
    "        #x = (x.float()/255.)#.unsqueeze(1).repeat(1,3,1,1)  #<class 'torch.Tensor'>\n",
    "        #x= x.permute(0,3,1,2) #[batchsize,w,h,channel] -> [batchsize, channel, w,h]\n",
    "        y = torch.tensor(y)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump([x, y], f)\n",
    "    with open(path, 'rb') as f:\n",
    "        x, y = pickle.load(f)\n",
    "        if channels == 1:\n",
    "            x = x[:,0:1,:,:]\n",
    "    \n",
    "    if ntr is not None:\n",
    "        x, y = x[0:ntr], y[0:ntr]\n",
    "    \n",
    "    # Without Data Augmentation\n",
    "    if (translate is None) and (autoaug is None):\n",
    "        dataset = TensorDataset(x, y)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ca7229f-4839-45d8-b898-9fee73fd74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_m(split='train', channels=3):\n",
    "    path = f'data/mnist_m-{split}.pkl'\n",
    "    with open(path, 'rb') as f:\n",
    "        x, y = pickle.load(f)\n",
    "        x, y = torch.tensor(x.astype('float32')/255.), torch.tensor(y)\n",
    "        if channels==1:\n",
    "            x = x.mean(1, keepdim=True)\n",
    "    dataset = TensorDataset(x, y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96a5e71d-03af-4388-a86e-8df9a1601375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_1(split='train', translate=None, twox=False, ntr=None, autoaug=None, channels=3):\n",
    "    path = f'data/cifar10_1-{split}.pkl'\n",
    "    cifar10_transforms_train= transforms.Compose([transforms.Resize((32,32))]) #224,224\n",
    "    if not os.path.exists(path):\n",
    "        dataset = tfds.as_numpy(tfds.load('cifar10_1', split= split, shuffle_files= True, batch_size= -1))\n",
    "        x, y = dataset['image'], dataset['label']\n",
    "        #dataset = CIFAR10(f'{HOME}/.pytorch/CIFAR10', train=(split=='train'), download=True, transform= cifar10_transforms_train)\n",
    "        #x, y = dataset.data, dataset.targets\n",
    "        \n",
    "        #Only Select First 10k images as train\n",
    "        #if split=='train':\n",
    "        #    x, y = x[0:10000], y[0:10000]\n",
    "        \n",
    "        #[TODO] - solve -> AttributeError: 'numpy.ndarray' object has no attribute 'numpy'\n",
    "        #x = torch.tensor(resize_imgs(x.numpy(), 32))\n",
    "        #x = torch.tensor(resize_imgs_dkcho(x, 32)) # x-> torch.Size([10000, 32, 32, 3])\n",
    "        x= torch.tensor(x)\n",
    "        x = (x.float()/255.)#.unsqueeze(1).repeat(1,3,1,1)  #<class 'torch.Tensor'>\n",
    "        x= x.permute(0,3,1,2) #[batchsize,w,h,channel] -> [batchsize, channel, w,h]\n",
    "        y = torch.tensor(y)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump([x, y], f)\n",
    "    with open(path, 'rb') as f:\n",
    "        x, y = pickle.load(f)\n",
    "        if channels == 1:\n",
    "            x = x[:,0:1,:,:]\n",
    "    \n",
    "    if ntr is not None:\n",
    "        x, y = x[0:ntr], y[0:ntr]\n",
    "    \n",
    "    # Without Data Augmentation\n",
    "    if (translate is None) and (autoaug is None):\n",
    "        dataset = TensorDataset(x, y)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "803af517-526c-420e-a731-67c5f6bf4dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10c_level(split='test', ctype= 'fog', level= 5, translate=None, twox=False, ntr=None, autoaug=None, channels=3):\n",
    "    path = f'data/cifar10c-{ctype}_{level}.pkl'\n",
    "    cifar10_transforms_train= transforms.Compose([transforms.Resize((32,32))]) #224,224\n",
    "    if not os.path.exists(path):\n",
    "        tfpath= f'cifar10_corrupted/{ctype}_{level}'.format(ctype= ctype, level= level)\n",
    "        dataset = tfds.as_numpy(tfds.load(tfpath, split= split, shuffle_files= True, batch_size= -1))\n",
    "        x, y = dataset['image'], dataset['label']\n",
    "        x= torch.tensor(x)\n",
    "        x = (x.float()/255.)#.unsqueeze(1).repeat(1,3,1,1)  #<class 'torch.Tensor'>\n",
    "        print(x.shape)\n",
    "        x= x.permute(0,3,1,2) #[batchsize,w,h,channel] -> [batchsize, channel, w,h]\n",
    "        y = torch.tensor(y)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump([x, y], f)\n",
    "    with open(path, 'rb') as f:\n",
    "        x, y = pickle.load(f)\n",
    "        if channels == 1:\n",
    "            x = x[:,0:1,:,:]\n",
    "    \n",
    "    if ntr is not None:\n",
    "        x, y = x[0:ntr], y[0:ntr]\n",
    "    \n",
    "    # Without Data Augmentation\n",
    "    if (translate is None) and (autoaug is None):\n",
    "        dataset = TensorDataset(x, y)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8333cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= load_cifar10('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cc92590",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset,batch_size=128,drop_last=True, shuffle=True)\n",
    "x, y= next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e160f720-0b73-456c-90ea-90cbdaaf0e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6f6e7-701b-460f-912e-24925ac64822",
   "metadata": {},
   "source": [
    "# CIFAR10c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b882771-405b-4dd5-88dd-819c0d0b1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds= tfds.load('cifar10_corrupted/fog_5', split= 'test', shuffle_files= True, batch_size= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28617106-c101-4bc8-8e03-780a69f31727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf265abc-c1d8-4bc5-89fe-068c180eb972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f355987-03ae-455e-9008-f33bb607c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corruption_type= ['fog','snow','frost','zoom_blur','defocus_blur','frosted_glass_blur','speckle_noise',\n",
    "                      'shot_noise','impulse_noise','jpeg_compression','pixelate','spatter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9178261-ad17-4b5c-a865-664c98e773a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "for c_type in corruption_type:\n",
    "    st= 'cifar10_corrupted/{c_type}_{lv}'.format(c_type= c_type, lv=5)\n",
    "    ds= tfds.load(st, split= 'test', shuffle_files= True, batch_size= -1)\n",
    "    print(len(ds['image']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "415ccdff-9241-45b5-bdc5-381f3c69bda9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c_type \u001b[38;5;129;01min\u001b[39;00m corruption_type:\n\u001b[1;32m      2\u001b[0m     df\u001b[38;5;241m=\u001b[39m load_cifar10c_level(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, ctype\u001b[38;5;241m=\u001b[39m c_type, level\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for c_type in corruption_type:\n",
    "    df= load_cifar10c_level(split='test', ctype= c_type, level= 5)\n",
    "    print(df[0][0][0][0])\n",
    "    print(\"===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7abb33e-8dea-4af1-aa0b-4043c0e427b0",
   "metadata": {},
   "source": [
    "# MNIST-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9deb271-bc73-4f1e-969f-67c7be1115b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= load_mnist_m('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000a9a8-fc43-4c0f-93a0-eefca4a9d8c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30c91c-9be7-4dec-8ea0-7136ded29172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/dongkyu/.pytorch/SVHN/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "dataset= load_svhn('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6aca5d-5918-430c-bdb1-8ab75bb4834e",
   "metadata": {},
   "source": [
    "# PACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39e5f70f-c11c-4996-bedf-9fff590bc960",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= load_pacs(split='photo')\n",
    "train_size= len(dataset)\n",
    "train_loader = torch.utils.data.DataLoader(dataset,batch_size=train_size,drop_last=True, shuffle=True)\n",
    "\n",
    "x, y= next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ba17711-ca20-4477-8ba4-1fe1cedebc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1670"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0fcf0631-6693-4e85-b00e-07312dec845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 816, 0: 772, 2: 753, 1: 740, 3: 608, 6: 160, 5: 80})\n"
     ]
    }
   ],
   "source": [
    "y= y.tolist()\n",
    "from collections import Counter\n",
    "c=Counter(y)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "07c6af8b-a70e-489d-b0e9-1baad6f0875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample,samplelabel= x[0],y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a53fd427-3b15-49f3-bc88-8e811af2ffc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samplelabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "10450e38-7c6e-479a-b591-7bcc3daec19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topil= transforms.ToPILImage()\n",
    "image= topil(sample)\n",
    "image.save('./data/image_test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6020f57-e6ae-4618-9d91-e313a70fd5b2",
   "metadata": {},
   "source": [
    "# Check STL10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e6491ccf-2aea-4b95-8616-c361298bc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stl10(split='train', channels=3):\n",
    "    STL10_transforms_train= transforms.Compose([transforms.Resize((32,32))])\n",
    "    dataset = STL10(f'{HOME}/.pytorch/STL10', split=split, download=True, transform= STL10_transforms_train)\n",
    "    x, y = dataset.data, dataset.labels\n",
    "    x = x.astype('float32')/255.\n",
    "    x, y = torch.tensor(x), torch.tensor(y)\n",
    "    if channels == 1:\n",
    "        x = x.mean(1, keepdim=True)\n",
    "    dataset = TensorDataset(x, y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "27a9ee61-5f7d-4472-9f60-af46870cf9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset= load_stl10('train')\n",
    "train_loader = torch.utils.data.DataLoader(dataset,batch_size=1,drop_last=True)\n",
    "x, y= next(iter(train_loader))\n",
    "x= x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6b080218-0407-4c42-ba97-d4bda38e3c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "topil= transforms.ToPILImage()\n",
    "image= topil(x)\n",
    "image.save('image_test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74665b-27b1-48e4-a5a4-11d1658c60db",
   "metadata": {},
   "source": [
    "# Check CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4f3f1622-6459-459e-a8a4-17b59f675eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= load_cifar10('train')\n",
    "train_loader = torch.utils.data.DataLoader(dataset,batch_size=2,drop_last=True)\n",
    "x, y= next(iter(train_loader))\n",
    "x= x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "784ae918-e16e-423f-a111-34536cf13878",
   "metadata": {},
   "outputs": [],
   "source": [
    "topil= transforms.ToPILImage()\n",
    "image= topil(x)\n",
    "image.save('./data/image_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757dd7d-6fff-45c0-ba17-a1fab4cab585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c6d08f4-145c-42ba-b6b5-a6b918735ed4",
   "metadata": {},
   "source": [
    "# Wide Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9065b64e-3968-4030-a9da-358b1f609a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/simclr/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdad2515-7b43-4cea-bb10-29ec448e7cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/simclr/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet50_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet50_2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\" to /home/dongkyu/.cache/torch/hub/checkpoints/wide_resnet50_2-95faca4d.pth\n",
      "100%|██████████| 132M/132M [00:06<00:00, 20.0MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.models.wide_resnet50_2(weights= torchvision.models.resnet.Wide_ResNet50_2_Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a84fe5-94b7-4fa2-8308-e1221489b63b",
   "metadata": {},
   "source": [
    "# Con_LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34bcf83a-a7ce-4b43-b5a9-9b6ec7af8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= torch.randn(128,128)\n",
    "b = torch.randn(128,128)\n",
    "c = torch.randn(128,128)\n",
    "d = torch.randn(128,128)\n",
    "features= [a,b,c,d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18d5e0e9-40e7-404d-b297-13fad15916e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "051a7330-45f4-40e0-b1e8-5f9531147c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    # return a flattened view of the off-diagonal elements of a square matrix\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26475963-5eed-40eb-b929-d71ff282cbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0--1 LOSS: 130.9720458984375\n",
      "0--2 LOSS: 130.98446655273438\n",
      "0--3 LOSS: 129.42559814453125\n",
      "1--0 LOSS: 130.9720458984375\n",
      "1--2 LOSS: 129.5026397705078\n",
      "1--3 LOSS: 128.0531768798828\n",
      "2--0 LOSS: 130.98446655273438\n",
      "2--1 LOSS: 129.5026397705078\n",
      "2--3 LOSS: 130.7725067138672\n",
      "3--0 LOSS: 129.42559814453125\n",
      "3--1 LOSS: 128.0531768798828\n",
      "3--2 LOSS: 130.7725067138672\n"
     ]
    }
   ],
   "source": [
    "total_loss= 0.0\n",
    "for p, anchor_feature in enumerate(features):\n",
    "    for q, contrast_feature in enumerate(features):\n",
    "        if p != q:\n",
    "            anchor_feature= (anchor_feature - anchor_feature.mean(0)) / anchor_feature.std(0) #torch.Size([256, 128])\n",
    "            contrast_feature = (contrast_feature - contrast_feature.mean(0)) / contrast_feature.std(0) #torch.Size([256, 128])\n",
    "            c= torch.matmul(anchor_feature.T, contrast_feature) \n",
    "            c.div_(batch_size)\n",
    "            on_diag = torch.diagonal(c).add_(-1).pow_(2).sum() # appr. 2~3\n",
    "            off_diag = off_diagonal(c).pow_(2).sum()\n",
    "                        \n",
    "            loss = on_diag + 0.0051 * off_diag\n",
    "            print(\"{a}--{b} LOSS: {c}\".format(a=p,b=q,c= loss))\n",
    "            total_loss += loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c416e997-80eb-4bee-ba18-9c405480ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fb5eb628-9186-4e19-b077-872027353113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320.5998"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53.4333 * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1597345a-fb95-4f81-b84d-de8953a88947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314.8008"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26.2334 * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d3c9e5ce-0d5f-4086-a075-398fc6b19866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "a= list(itertools.combinations(list(range(len(features))), 2))\n",
    "if (1,2) in a:\n",
    "    print(\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0cabd532-27b2-4d57-9c2a-a66fe94e40ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c8d2a38c-315c-48d5-9e3e-ad26fed04349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [1, 3],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.combinations(torch.tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e98acd8-35c4-4be9-a1de-0eb7ca012065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unbind(features, dim=1)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2be69e9d-81ad-4cfd-8c19-56294ea1e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_count = features.shape[1]\n",
    "contrast_feature= torch.cat(torch.unbind(features, dim=1), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c42649d-97c9-4ff8-8433-628a0940a73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7332, 0.0929, 0.2580,  ..., 0.3973, 0.9955, 0.8535],\n",
       "        [0.0841, 0.8132, 0.7651,  ..., 0.0038, 0.9857, 0.8988],\n",
       "        [0.0678, 0.3891, 0.4295,  ..., 0.6152, 0.9006, 0.6406],\n",
       "        ...,\n",
       "        [0.4338, 0.5726, 0.2865,  ..., 0.3523, 0.1264, 0.3898],\n",
       "        [0.1844, 0.9726, 0.0948,  ..., 0.2757, 0.4321, 0.1781],\n",
       "        [0.0125, 0.1082, 0.3442,  ..., 0.6851, 0.8039, 0.7882]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce479d79-7e84-47b2-b874-a0088e259121",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_feature = features[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46b633bc-971c-4143-be84-94c67417cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_feature = contrast_feature\n",
    "anchor_count = contrast_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bde9c91a-6635-4733-a686-05d1174818f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fe154c3-da98-4ddb-9200-f48c48499e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[596.3107, 452.5212, 462.6601,  ..., 410.0806, 430.4772, 447.1158],\n",
       "        [452.5212, 615.8022, 454.5374,  ..., 439.8339, 446.4165, 479.2466],\n",
       "        [462.6601, 454.5374, 627.1229,  ..., 443.9037, 501.7697, 459.4048],\n",
       "        ...,\n",
       "        [410.0806, 439.8339, 443.9037,  ..., 558.7839, 472.4926, 447.3414],\n",
       "        [430.4772, 446.4165, 501.7697,  ..., 472.4926, 632.8708, 477.7922],\n",
       "        [447.1158, 479.2466, 459.4048,  ..., 447.3414, 477.7922, 637.1094]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_dot_contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b193b7a2-5876-4802-ae1a-cad9a2ba7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dbe3bfe-ffcd-4973-b41c-3deb1ed04bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits= anchor_dot_contrast- logits_max.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d174abf-9ca4-4b97-acd0-13fe83cee3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5b50e0d-458c-44b8-9184-026055d40bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask= mask.repeat(anchor_count, contrast_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5310c8b-f621-42d2-adbf-07f4fd553be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_mask = torch.scatter(torch.ones_like(mask), 1, torch.arange(batch_size*anchor_count).view(-1,1),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50aa907f-59fd-4d11-8c07-6a23a572d339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 0.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 0., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a60eb72a-edb9-48bf-9310-747171659e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask= mask*logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ba4493b-ef6a-4df6-9f0f-550d3c381af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_logits = torch.exp(logits) * logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6328d4bf-e3eb-44b6-9cda-e7e7ca32414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = torch.log( 1- exp_logits / (exp_logits.sum(1, keepdim=True)+1e-6) - 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0989deca-adb4-4cbd-8a46-2cd51cf270ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0133e-06, -1.0133e-06, -1.0133e-06,  ..., -1.0133e-06,\n",
       "         -1.0133e-06, -1.0133e-06],\n",
       "        [-1.0133e-06, -1.0133e-06, -1.0133e-06,  ..., -1.0133e-06,\n",
       "         -1.0133e-06, -1.0133e-06],\n",
       "        [-1.0133e-06, -1.0133e-06, -1.0133e-06,  ..., -1.0133e-06,\n",
       "         -1.0133e-06, -1.0133e-06],\n",
       "        ...,\n",
       "        [-1.0133e-06, -1.0133e-06, -1.0133e-06,  ..., -1.0133e-06,\n",
       "         -1.0133e-06, -1.0133e-06],\n",
       "        [-1.0133e-06, -1.0133e-06, -1.0133e-06,  ..., -1.0133e-06,\n",
       "         -1.0133e-06, -1.0133e-06],\n",
       "        [-1.0133e-06, -1.0133e-06, -1.0133e-06,  ..., -1.0133e-06,\n",
       "         -1.0133e-06, -1.0133e-06]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffdf59f-8162-44b1-970e-2a9c95a112d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simclr",
   "language": "python",
   "name": "simclr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
