{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37170982-68d1-4505-bb81-495a00375b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dongkyu/sDG\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f575f57-77ba-41fc-b358-3b8c1b457305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/simclr/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, USPS, SVHN, CIFAR10, STL10\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "from tools.autoaugment import SVHNPolicy, CIFAR10Policy\n",
    "from tools.randaugment import RandAugment\n",
    "\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7003b425-81b8-4419-b336-6f87ba949dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.environ['HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27304e8f-6841-49ba-9d6c-43b4962abfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10(split='train', translate=None, twox=False, ntr=None, autoaug=None, channels=3):\n",
    "    '''\n",
    "        autoaug == 'AA', AutoAugment\n",
    "                   'FastAA', Fast AutoAugment\n",
    "                   'RA', RandAugment\n",
    "        channels == 3 return by default rgb 3 channel image\n",
    "                    1 Return a single channel image\n",
    "    '''\n",
    "    path = f'data/cifar10-{split}.pkl'\n",
    "    cifar10_transforms_train= transforms.Compose([transforms.Resize((32,32))]) #224,224\n",
    "    if not os.path.exists(path):\n",
    "        dataset = CIFAR10(f'{HOME}/.pytorch/CIFAR10', train=(split=='train'), download=True, transform= cifar10_transforms_train)\n",
    "        x, y = dataset.data, dataset.targets\n",
    "        \n",
    "        #Only Select First 10k images as train\n",
    "        #if split=='train':\n",
    "        #    x, y = x[0:10000], y[0:10000]\n",
    "        \n",
    "        #[TODO] - solve -> AttributeError: 'numpy.ndarray' object has no attribute 'numpy'\n",
    "        #x = torch.tensor(resize_imgs(x.numpy(), 32))\n",
    "        #x = torch.tensor(resize_imgs_dkcho(x, 32)) # x-> torch.Size([10000, 32, 32, 3])\n",
    "        x= torch.tensor(x)\n",
    "        x = (x.float()/255.)#.unsqueeze(1).repeat(1,3,1,1)  #<class 'torch.Tensor'>\n",
    "        x= x.permute(0,3,1,2) #[batchsize,w,h,channel] -> [batchsize, channel, w,h]\n",
    "        y = torch.tensor(y)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump([x, y], f)\n",
    "    with open(path, 'rb') as f:\n",
    "        x, y = pickle.load(f)\n",
    "        if channels == 1:\n",
    "            x = x[:,0:1,:,:]\n",
    "    \n",
    "    if ntr is not None:\n",
    "        x, y = x[0:ntr], y[0:ntr]\n",
    "    \n",
    "    # Without Data Augmentation\n",
    "    if (translate is None) and (autoaug is None):\n",
    "        dataset = TensorDataset(x, y)\n",
    "        return dataset\n",
    "    \n",
    "    # Data Augmentation Pipeline\n",
    "    transform = [transforms.ToPILImage()]\n",
    "    if translate is not None:\n",
    "        transform.append(transforms.RandomAffine(0, [translate, translate]))\n",
    "    if autoaug is not None:\n",
    "        if autoaug == 'AA':\n",
    "            transform.append(CIFAR10Policy()) #originally SVHNPolicy()\n",
    "        elif autoaug == 'RA':\n",
    "            transform.append(RandAugment(3,4))\n",
    "    transform.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(transform)\n",
    "    dataset = myTensorDataset(x, y, transform=transform, twox=twox)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_usps(split='train', channels=3):\n",
    "    path = f'data/usps-{split}.pkl'\n",
    "    if not os.path.exists(path):\n",
    "        dataset = USPS(f'{HOME}/.pytorch/USPS', train=(split=='train'), download=True)\n",
    "        x, y = dataset.data, dataset.targets\n",
    "        x = torch.tensor(resize_imgs(x, 32))\n",
    "        x = (x.float()/255.).unsqueeze(1).repeat(1,3,1,1)\n",
    "        y = torch.tensor(y)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump([x, y], f)\n",
    "    with open(path, 'rb') as f:\n",
    "        x, y = pickle.load(f)\n",
    "        if channels == 1:\n",
    "            x = x[:,0:1,:,:]\n",
    "    dataset = TensorDataset(x, y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b43914-8bfc-428d-b0f9-8c75119b6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10c(split='train', translate=None, twox=False, ntr=None, autoaug=None, channels=3):\n",
    "    path = f'data/cifar10c-{split}.pkl'\n",
    "    cifar10_transforms_train= transforms.Compose([transforms.Resize((32,32))]) #224,224\n",
    "    if not os.path.exists(path):\n",
    "        dataset = tfds.as_numpy(tfds.load('cifar10_corrupted', split= split, shuffle_files= True, batch_size= -1))\n",
    "        x, y = dataset['image'], dataset['label']\n",
    "        x= torch.tensor(x)\n",
    "        x = (x.float()/255.)#.unsqueeze(1).repeat(1,3,1,1)  #<class 'torch.Tensor'>\n",
    "        x= x.permute(0,3,1,2) #[batchsize,w,h,channel] -> [batchsize, channel, w,h]\n",
    "        y = torch.tensor(y)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump([x, y], f)\n",
    "    with open(path, 'rb') as f:\n",
    "        x, y = pickle.load(f)\n",
    "        if channels == 1:\n",
    "            x = x[:,0:1,:,:]\n",
    "    \n",
    "    if ntr is not None:\n",
    "        x, y = x[0:ntr], y[0:ntr]\n",
    "    \n",
    "    # Without Data Augmentation\n",
    "    if (translate is None) and (autoaug is None):\n",
    "        dataset = TensorDataset(x, y)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bde2452-7878-40e1-9941-0b591678f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv={0:\"airplane\",\n",
    "1:\"automobile\",\n",
    "2:\"bird\",\n",
    "3:\"cat\",\n",
    "4:\"deer\",\n",
    "5:\"dog\",\n",
    "6:\"frog\",\n",
    "7:\"horse\",\n",
    "8:\"ship\",\n",
    "9:\"truck\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5e1344d7-2633-4c7c-bd20-eadb9d87f3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_transforms_train = transforms.Compose([transforms.ToTensor()])#, transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "dataset = CIFAR10(f'{HOME}/.pytorch/CIFAR10', train='train', download=True, transform= cifar10_transforms_train)#cifar10_transforms_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dff67791-7401-4c1c-a637-8027a6b6bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset.data, dataset.targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "23197362-fcc1-4b8f-9fce-a0a728c50e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.tensor(x)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d006d012-08ac-4174-b320-f654343dfb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= x.float()\n",
    "#x= (x.float()/255.)\n",
    "x= x.permute(0,3,1,2) #[batchsize,w,h,channel] -> [batchsize, channel, w,h]\n",
    "\n",
    "#x= transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2c73f032-458f-41cb-b018-1fd3a6a25902",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= TensorDataset(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c94d4067-ba8f-417d-a5e6-84cb101246ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset,batch_size=128,drop_last=True, shuffle=True)\n",
    "x, y= next(iter(train_loader))\n",
    "\n",
    "i= 77\n",
    "sample= x[i]\n",
    "samplelabel= y[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0b5e0dc6-f282-4066-927b-6b874c3c94ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x= x.permute(0,2,3,1)\n",
    "\n",
    "# define a transform to convert a tensor to PIL image\n",
    "transform = T.ToPILImage()\n",
    "\n",
    "# convert the tensor to PIL image using above transform\n",
    "img = transform(sample)\n",
    "\n",
    "# display the PIL image\n",
    "img.show()\n",
    "\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9b2eb0ad-aae0-4cea-a73c-9de7862c5350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({9: 15, 2: 10, 6: 15, 5: 9, 3: 16, 0: 8, 1: 21, 8: 12, 7: 8, 4: 14})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6f6e7-701b-460f-912e-24925ac64822",
   "metadata": {},
   "source": [
    "# CIFAR10c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b882771-405b-4dd5-88dd-819c0d0b1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds= tfds.load('cifar10_corrupted/impulse_noise_5', split= 'test', shuffle_files= False, batch_size= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28617106-c101-4bc8-8e03-780a69f31727",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Scalar tensor has no `len()`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/simclr/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1104\u001b[0m, in \u001b[0;36m_EagerTensorBase.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the length of the first dimension in the Tensor.\"\"\"\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mndims:\n\u001b[0;32m-> 1104\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScalar tensor has no `len()`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Scalar tensor has no `len()`"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ccdff-9241-45b5-bdc5-381f3c69bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(to_pil_image(img), cmap='gray')\n",
    "plt.title('train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7abb33e-8dea-4af1-aa0b-4043c0e427b0",
   "metadata": {},
   "source": [
    "# MNIST-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9deb271-bc73-4f1e-969f-67c7be1115b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= load_mnist_m('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000a9a8-fc43-4c0f-93a0-eefca4a9d8c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30c91c-9be7-4dec-8ea0-7136ded29172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/dongkyu/.pytorch/SVHN/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "dataset= load_svhn('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6aca5d-5918-430c-bdb1-8ab75bb4834e",
   "metadata": {},
   "source": [
    "# PACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39e5f70f-c11c-4996-bedf-9fff590bc960",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= load_pacs(split='photo')\n",
    "train_size= len(dataset)\n",
    "train_loader = torch.utils.data.DataLoader(dataset,batch_size=train_size,drop_last=True, shuffle=True)\n",
    "\n",
    "x, y= next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ba17711-ca20-4477-8ba4-1fe1cedebc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1670"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0fcf0631-6693-4e85-b00e-07312dec845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 816, 0: 772, 2: 753, 1: 740, 3: 608, 6: 160, 5: 80})\n"
     ]
    }
   ],
   "source": [
    "y= y.tolist()\n",
    "from collections import Counter\n",
    "c=Counter(y)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "07c6af8b-a70e-489d-b0e9-1baad6f0875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample,samplelabel= x[0],y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a53fd427-3b15-49f3-bc88-8e811af2ffc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samplelabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "10450e38-7c6e-479a-b591-7bcc3daec19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topil= transforms.ToPILImage()\n",
    "image= topil(sample)\n",
    "image.save('./data/image_test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6020f57-e6ae-4618-9d91-e313a70fd5b2",
   "metadata": {},
   "source": [
    "# Check STL10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e6491ccf-2aea-4b95-8616-c361298bc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stl10(split='train', channels=3):\n",
    "    STL10_transforms_train= transforms.Compose([transforms.Resize((32,32))])\n",
    "    dataset = STL10(f'{HOME}/.pytorch/STL10', split=split, download=True, transform= STL10_transforms_train)\n",
    "    x, y = dataset.data, dataset.labels\n",
    "    x = x.astype('float32')/255.\n",
    "    x, y = torch.tensor(x), torch.tensor(y)\n",
    "    if channels == 1:\n",
    "        x = x.mean(1, keepdim=True)\n",
    "    dataset = TensorDataset(x, y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "27a9ee61-5f7d-4472-9f60-af46870cf9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset= load_stl10('train')\n",
    "train_loader = torch.utils.data.DataLoader(dataset,batch_size=1,drop_last=True)\n",
    "x, y= next(iter(train_loader))\n",
    "x= x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6b080218-0407-4c42-ba97-d4bda38e3c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "topil= transforms.ToPILImage()\n",
    "image= topil(x)\n",
    "image.save('image_test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74665b-27b1-48e4-a5a4-11d1658c60db",
   "metadata": {},
   "source": [
    "# Check CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4f3f1622-6459-459e-a8a4-17b59f675eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= load_cifar10('train')\n",
    "train_loader = torch.utils.data.DataLoader(dataset,batch_size=2,drop_last=True)\n",
    "x, y= next(iter(train_loader))\n",
    "x= x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "784ae918-e16e-423f-a111-34536cf13878",
   "metadata": {},
   "outputs": [],
   "source": [
    "topil= transforms.ToPILImage()\n",
    "image= topil(x)\n",
    "image.save('./data/image_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757dd7d-6fff-45c0-ba17-a1fab4cab585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c6d08f4-145c-42ba-b6b5-a6b918735ed4",
   "metadata": {},
   "source": [
    "# Wide Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9065b64e-3968-4030-a9da-358b1f609a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/simclr/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdad2515-7b43-4cea-bb10-29ec448e7cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/simclr/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=Wide_ResNet50_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet50_2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\" to /home/dongkyu/.cache/torch/hub/checkpoints/wide_resnet50_2-95faca4d.pth\n",
      "100%|██████████| 132M/132M [00:06<00:00, 20.0MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.models.wide_resnet50_2(weights= torchvision.models.resnet.Wide_ResNet50_2_Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a84fe5-94b7-4fa2-8308-e1221489b63b",
   "metadata": {},
   "source": [
    "# Con_LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34bcf83a-a7ce-4b43-b5a9-9b6ec7af8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= torch.randn(128,128)\n",
    "b = torch.randn(128,128)\n",
    "c = torch.randn(128,128)\n",
    "d = torch.randn(128,128)\n",
    "features= [a,b,c,d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18d5e0e9-40e7-404d-b297-13fad15916e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "051a7330-45f4-40e0-b1e8-5f9531147c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    # return a flattened view of the off-diagonal elements of a square matrix\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26475963-5eed-40eb-b929-d71ff282cbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0--1 LOSS: 130.9720458984375\n",
      "0--2 LOSS: 130.98446655273438\n",
      "0--3 LOSS: 129.42559814453125\n",
      "1--0 LOSS: 130.9720458984375\n",
      "1--2 LOSS: 129.5026397705078\n",
      "1--3 LOSS: 128.0531768798828\n",
      "2--0 LOSS: 130.98446655273438\n",
      "2--1 LOSS: 129.5026397705078\n",
      "2--3 LOSS: 130.7725067138672\n",
      "3--0 LOSS: 129.42559814453125\n",
      "3--1 LOSS: 128.0531768798828\n",
      "3--2 LOSS: 130.7725067138672\n"
     ]
    }
   ],
   "source": [
    "total_loss= 0.0\n",
    "for p, anchor_feature in enumerate(features):\n",
    "    for q, contrast_feature in enumerate(features):\n",
    "        if p != q:\n",
    "            anchor_feature= (anchor_feature - anchor_feature.mean(0)) / anchor_feature.std(0) #torch.Size([256, 128])\n",
    "            contrast_feature = (contrast_feature - contrast_feature.mean(0)) / contrast_feature.std(0) #torch.Size([256, 128])\n",
    "            c= torch.matmul(anchor_feature.T, contrast_feature) \n",
    "            c.div_(batch_size)\n",
    "            on_diag = torch.diagonal(c).add_(-1).pow_(2).sum() # appr. 2~3\n",
    "            off_diag = off_diagonal(c).pow_(2).sum()\n",
    "                        \n",
    "            loss = on_diag + 0.0051 * off_diag\n",
    "            print(\"{a}--{b} LOSS: {c}\".format(a=p,b=q,c= loss))\n",
    "            total_loss += loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c416e997-80eb-4bee-ba18-9c405480ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fb5eb628-9186-4e19-b077-872027353113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320.5998"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53.4333 * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1597345a-fb95-4f81-b84d-de8953a88947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314.8008"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26.2334 * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d3c9e5ce-0d5f-4086-a075-398fc6b19866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "a= list(itertools.combinations(list(range(len(features))), 2))\n",
    "if (1,2) in a:\n",
    "    print(\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0cabd532-27b2-4d57-9c2a-a66fe94e40ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c8d2a38c-315c-48d5-9e3e-ad26fed04349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [1, 3],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.combinations(torch.tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e98acd8-35c4-4be9-a1de-0eb7ca012065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unbind(features, dim=1)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2be69e9d-81ad-4cfd-8c19-56294ea1e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_count = features.shape[1]\n",
    "contrast_feature= torch.cat(torch.unbind(features, dim=1), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c42649d-97c9-4ff8-8433-628a0940a73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7332, 0.0929, 0.2580,  ..., 0.3973, 0.9955, 0.8535],\n",
       "        [0.0841, 0.8132, 0.7651,  ..., 0.0038, 0.9857, 0.8988],\n",
       "        [0.0678, 0.3891, 0.4295,  ..., 0.6152, 0.9006, 0.6406],\n",
       "        ...,\n",
       "        [0.4338, 0.5726, 0.2865,  ..., 0.3523, 0.1264, 0.3898],\n",
       "        [0.1844, 0.9726, 0.0948,  ..., 0.2757, 0.4321, 0.1781],\n",
       "        [0.0125, 0.1082, 0.3442,  ..., 0.6851, 0.8039, 0.7882]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce479d79-7e84-47b2-b874-a0088e259121",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_feature = features[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46b633bc-971c-4143-be84-94c67417cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_feature = contrast_feature\n",
    "anchor_count = contrast_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bde9c91a-6635-4733-a686-05d1174818f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fe154c3-da98-4ddb-9200-f48c48499e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[596.3107, 452.5212, 462.6601,  ..., 410.0806, 430.4772, 447.1158],\n",
       "        [452.5212, 615.8022, 454.5374,  ..., 439.8339, 446.4165, 479.2466],\n",
       "        [462.6601, 454.5374, 627.1229,  ..., 443.9037, 501.7697, 459.4048],\n",
       "        ...,\n",
       "        [410.0806, 439.8339, 443.9037,  ..., 558.7839, 472.4926, 447.3414],\n",
       "        [430.4772, 446.4165, 501.7697,  ..., 472.4926, 632.8708, 477.7922],\n",
       "        [447.1158, 479.2466, 459.4048,  ..., 447.3414, 477.7922, 637.1094]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_dot_contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b193b7a2-5876-4802-ae1a-cad9a2ba7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dbe3bfe-ffcd-4973-b41c-3deb1ed04bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits= anchor_dot_contrast- logits_max.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d174abf-9ca4-4b97-acd0-13fe83cee3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5b50e0d-458c-44b8-9184-026055d40bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask= mask.repeat(anchor_count, contrast_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5310c8b-f621-42d2-adbf-07f4fd553be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_mask = torch.scatter(torch.ones_like(mask), 1, torch.arange(batch_size*anchor_count).view(-1,1),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50aa907f-59fd-4d11-8c07-6a23a572d339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 0.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 0., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a60eb72a-edb9-48bf-9310-747171659e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask= mask*logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ba4493b-ef6a-4df6-9f0f-550d3c381af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_logits = torch.exp(logits) * logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6328d4bf-e3eb-44b6-9cda-e7e7ca32414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = torch.log( 1- exp_logits / (exp_logits.sum(1, keepdim=True)+1e-6) - 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0989deca-adb4-4cbd-8a46-2cd51cf270ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0133e-06, -1.0133e-06, -1.0133e-06,  ..., -1.0133e-06,\n",
       "         -1.0133e-06, -1.0133e-06],\n",
       "        [-1.0133e-06, -1.0133e-06, -1.0133e-06,  ..., -1.0133e-06,\n",
       "         -1.0133e-06, -1.0133e-06],\n",
       "        [-1.0133e-06, -1.0133e-06, -1.0133e-06,  ..., -1.0133e-06,\n",
       "         -1.0133e-06, -1.0133e-06],\n",
       "        ...,\n",
       "        [-1.0133e-06, -1.0133e-06, -1.0133e-06,  ..., -1.0133e-06,\n",
       "         -1.0133e-06, -1.0133e-06],\n",
       "        [-1.0133e-06, -1.0133e-06, -1.0133e-06,  ..., -1.0133e-06,\n",
       "         -1.0133e-06, -1.0133e-06],\n",
       "        [-1.0133e-06, -1.0133e-06, -1.0133e-06,  ..., -1.0133e-06,\n",
       "         -1.0133e-06, -1.0133e-06]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffdf59f-8162-44b1-970e-2a9c95a112d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simclr",
   "language": "python",
   "name": "simclr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
