{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e37a75-1fdd-4844-a4f1-5acf0f1c6c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, USPS, SVHN, CIFAR10, STL10, ImageFolder\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "\n",
    "from tools.autoaugment import SVHNPolicy, CIFAR10Policy\n",
    "from tools.randaugment import RandAugment\n",
    "\n",
    "HOME = os.environ['HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "022140e8-954d-4749-94a7-ac7ed39545d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_officehome(split='train', translate=None, twox=False, ntr=None, autoaug=None, channels=3):\n",
    "    DIR_REALWORLD = './data/office_home/Real World'\n",
    "    trainpath= 'data/officehome-train.pkl'\n",
    "    testpath= 'data/officehome-test.pkl'\n",
    "    path = f'data/officehome-{split}.pkl'\n",
    "    officehome_convertor= {'train':trainpath, 'test':testpath}\n",
    "    officehome_transforms_train= transforms.Compose([transforms.CenterCrop(224),transforms.Resize((224,224)),transforms.ToTensor()]) #224,224\n",
    "    if not os.path.exists(path):\n",
    "        \n",
    "        dataset= ImageFolder(DIR_REALWORLD, transform=officehome_transforms_train)\n",
    "        \n",
    "        train_size = int(0.7 * len(dataset))\n",
    "        test_size = len(dataset) - train_size\n",
    "        train_set, test_set = random_split(dataset, [train_size, test_size])\n",
    "        \n",
    "        #Train Set\n",
    "        train_loader = torch.utils.data.DataLoader(train_set,batch_size=train_size,drop_last=True)\n",
    "        x, y= next(iter(train_loader))\n",
    "        x= torch.tensor(x)\n",
    "        y = torch.tensor(y)\n",
    "        with open(trainpath, 'wb') as f:\n",
    "            pickle.dump([x, y], f)\n",
    "        \n",
    "        #Test Set\n",
    "        test_loader = torch.utils.data.DataLoader(test_set,batch_size=test_size,drop_last=True, shuffle=True)\n",
    "        x, y= next(iter(test_loader))\n",
    "        x= torch.tensor(x)\n",
    "        y = torch.tensor(y)\n",
    "        with open(testpath, 'wb') as f:\n",
    "            pickle.dump([x, y], f) \n",
    "\n",
    "    with open(path, 'rb') as f:\n",
    "        x, y = pickle.load(f)\n",
    "        if channels == 1:\n",
    "            x = x[:,0:1,:,:]\n",
    "    \n",
    "    if ntr is not None:\n",
    "        x, y = x[0:ntr], y[0:ntr]\n",
    "    \n",
    "    # Without Data Augmentation\n",
    "    if (translate is None) and (autoaug is None):\n",
    "        dataset = TensorDataset(x, y)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4d5cd4c-d0c6-438d-a9a7-e7e205615ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_officehome_domain(domain='Product', translate=None, twox=False, ntr=None, autoaug=None, channels=3):\n",
    "    DATASETS_NAMES = ['Art', 'Clipart', 'Product', 'Real World']\n",
    "    DIR_Art = './data/office_home/Art'\n",
    "    DIR_Clipart = './data/office_home/Clipart'\n",
    "    DIR_Product = './data/office_home/Product'\n",
    "    DIR_REALWORLD = './data/office_home/Real World'\n",
    "\n",
    "    path = f'data/officehome-{domain}.pkl'\n",
    "    #trainpath= 'data/officehome-train.pkl'\n",
    "    #testpath= 'data/officehome-test.pkl'\n",
    "\n",
    "    officehome_convertor= {'Art':DIR_Art, 'Clipart':DIR_Clipart,'Product':DIR_Product,'Real World':DIR_REALWORLD}\n",
    "    officehome_transforms_train= transforms.Compose([transforms.CenterCrop(224),transforms.Resize((224,224)),transforms.ToTensor()]) #224,224\n",
    "    if not os.path.exists(path):\n",
    "        \n",
    "        dataset= ImageFolder(officehome_convertor[domain], transform=officehome_transforms_train)\n",
    "        test_size = len(dataset)\n",
    "\n",
    "        #Test Set\n",
    "        test_loader = torch.utils.data.DataLoader(dataset,batch_size=test_size)\n",
    "        x, y= next(iter(test_loader))\n",
    "        x= torch.tensor(x)\n",
    "        y = torch.tensor(y)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump([x, y], f)\n",
    "\n",
    "    with open(path, 'rb') as f:\n",
    "        x, y = pickle.load(f)\n",
    "        if channels == 1:\n",
    "            x = x[:,0:1,:,:]\n",
    "    \n",
    "    if ntr is not None:\n",
    "        x, y = x[0:ntr], y[0:ntr]\n",
    "    \n",
    "    # Without Data Augmentation\n",
    "    if (translate is None) and (autoaug is None):\n",
    "        dataset = TensorDataset(x, y)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "907089c4-2cf9-4100-a46f-129ae0c1a526",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mload_officehome\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [17], line 5\u001b[0m, in \u001b[0;36mload_officehome\u001b[0;34m(split, translate, twox, ntr, autoaug, channels)\u001b[0m\n\u001b[1;32m      3\u001b[0m trainpath\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/officehome-train.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m testpath\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/officehome-test.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/officehome-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(c\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m      6\u001b[0m officehome_convertor\u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:trainpath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m:testpath}\n\u001b[1;32m      7\u001b[0m officehome_transforms_train\u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mCenterCrop(\u001b[38;5;241m224\u001b[39m),transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m)),transforms\u001b[38;5;241m.\u001b[39mToTensor()]) \u001b[38;5;66;03m#224,224\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "load_officehome('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b4b06-a24d-45d1-beac-49e64d987301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simclr",
   "language": "python",
   "name": "simclr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
